{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Search integrated vectorization over a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --pre azure-search-documents --quiet\n",
    "! pip install azure-identity azure-storage-blob openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    AzureOpenAIModelName,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters,\n",
    "    BlobIndexerParsingMode,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    IndexerExecutionStatus,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataSourceType,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.models import VectorizableTextQuery, VectorizedQuery\n",
    "from azure.storage.blob import BlobClient, BlobServiceClient, ContainerClient\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import os\n",
    "from openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Environment Variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = \"2024-02-01\"  \n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\")\n",
    "BLOB_RESOURCE_ID = os.getenv(\"BLOB_RESOURCE_ID\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\")\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")  \n",
    "INDEX_NAME = \"csv-sample\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication for Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AAD for authentication.\n"
     ]
    }
   ],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_AOAI = True\n",
    "\n",
    "def authenticate_openai(api_key=None, use_aad_for_aoai=False):\n",
    "    from azure.identity import get_bearer_token_provider\n",
    "    from openai import AzureOpenAI\n",
    "\n",
    "    if use_aad_for_aoai:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "        token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "            api_version=AZURE_OPENAI_API_VERSION,\n",
    "            azure_ad_token_provider=token_provider,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        client = AzureOpenAI(\n",
    "            api_key=api_key,\n",
    "            api_version=AZURE_OPENAI_API_VERSION,\n",
    "            azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        )\n",
    "    return client\n",
    "\n",
    "openai_client = authenticate_openai(api_key=AZURE_OPENAI_API_KEY, use_aad_for_aoai=USE_AAD_FOR_AOAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication for Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AAD for authentication.\n"
     ]
    }
   ],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = True  \n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=SEARCH_SERVICE_API_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The specified container already exists.\n",
      "RequestId:4d1f262e-301e-0066-425a-b4a9f0000000\n",
      "Time:2024-06-01T19:29:49.2680683Z\n",
      "ErrorCode:ContainerAlreadyExists\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
      "RequestId:4d1f262e-301e-0066-425a-b4a9f0000000\n",
      "Time:2024-06-01T19:29:49.2680683Z</Message></Error>\n"
     ]
    }
   ],
   "source": [
    "def upload_file_to_blob(connection_string, container_name, file_path):\n",
    "    \"\"\"Upload a file to the specified blob container.\"\"\"\n",
    "    try:\n",
    "        # Initialize the BlobServiceClient\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "        # Get the container client\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "        # Create the container if it doesn't exist\n",
    "        container_client.create_container()\n",
    "\n",
    "        # Upload the file\n",
    "        file_name = os.path.basename(file_path)\n",
    "        blob_client = container_client.get_blob_client(file_name)\n",
    "        with open(file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "        print(f\"Uploaded blob: {file_name} to container: {container_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Main workflow\n",
    "CSV_FILE_PATH = os.path.join(\"data\", \"csv\", \"AG_news_samples.csv\")\n",
    "\n",
    "upload_file_to_blob(BLOB_CONNECTION_STRING, BLOB_CONTAINER_NAME, CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Blob Data Source Connector on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'csv-sample-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "def create_or_update_data_source(indexer_client, container_name, resource_id, index_name):\n",
    "    \"\"\"Create or update a data source connection for Azure Cognitive Search using a connection string. \"\"\"\n",
    "    try:\n",
    "        container = SearchIndexerDataContainer(name=container_name)\n",
    "\n",
    "        data_source_connection = SearchIndexerDataSourceConnection(\n",
    "            name=f\"{index_name}-blob\",\n",
    "            type=SearchIndexerDataSourceType.AZURE_BLOB,\n",
    "            connection_string=resource_id,\n",
    "            container=container\n",
    "        )\n",
    "        data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "        print(f\"Data source '{data_source.name}' created or updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create or update data source: {e}\")\n",
    "\n",
    "# Initialize the SearchIndexerClient with a credential\n",
    "indexer_client = SearchIndexerClient(SEARCH_SERVICE_ENDPOINT, azure_search_credential)\n",
    "\n",
    "# Call the function to create or update the data source\n",
    "create_or_update_data_source(indexer_client, BLOB_CONTAINER_NAME, BLOB_RESOURCE_ID, INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv-sample created\n"
     ]
    }
   ],
   "source": [
    "def create_fields():\n",
    "    \"\"\"Creates the fields for the search index based on the specified schema.\"\"\"\n",
    "    return [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True),\n",
    "        SearchField(name=\"title\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"description\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"label\", type=SearchFieldDataType.String, facetable=True,filterable=True),\n",
    "        SearchField(name=\"label_int\", type=SearchFieldDataType.Int32, sortable=True, filterable=True, facetable=True),\n",
    "        SearchField(\n",
    "            name=\"vector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            hidden=False,\n",
    "            stored=True\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "def create_vector_search_configuration():\n",
    "    \"\"\"Creates the vector search configuration.\"\"\"\n",
    "    return VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "            ExhaustiveKnnAlgorithmConfiguration(\n",
    "                name=\"myExhaustiveKnn\",\n",
    "                parameters=ExhaustiveKnnParameters(\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "                vectorizer=\"myOpenAI\",\n",
    "            ),\n",
    "            VectorSearchProfile(\n",
    "                name=\"myExhaustiveKnnProfile\",\n",
    "                algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "                vectorizer=\"myOpenAI\",\n",
    "            ),\n",
    "        ],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                name=\"myOpenAI\",\n",
    "                kind=\"azureOpenAI\",\n",
    "                azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                    resource_uri=AZURE_OPENAI_ENDPOINT,\n",
    "                    deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "                    api_key=AZURE_OPENAI_API_KEY,\n",
    "                    model_name=AzureOpenAIModelName.TEXT_EMBEDDING3_LARGE\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def create_semantic_search_configuration():\n",
    "    \"\"\"Creates the semantic search configuration.\"\"\"\n",
    "    return SemanticSearch(configurations=[\n",
    "        SemanticConfiguration(\n",
    "            name=\"mySemanticConfig\",\n",
    "            prioritized_fields=SemanticPrioritizedFields(\n",
    "                title_field=SemanticField(field_name=\"title\"),\n",
    "                content_fields=[SemanticField(field_name=\"description\")]\n",
    "            ),\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def create_search_index(index_name, fields, vector_search, semantic_search):\n",
    "    \"\"\"Creates or updates the search index.\"\"\"\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_search=semantic_search\n",
    "    )\n",
    "    try:\n",
    "        result = index_client.create_or_update_index(index)\n",
    "        print(f\"{result.name} created\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create or update index: {e}\")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=SEARCH_SERVICE_ENDPOINT, credential=azure_search_credential)\n",
    "fields = create_fields()\n",
    "vector_search = create_vector_search_configuration()\n",
    "semantic_search = create_semantic_search_configuration()\n",
    "\n",
    "# Create the search index with the adjusted schema\n",
    "create_search_index(INDEX_NAME, fields, vector_search, semantic_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv-sample-skillset created\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_skill(azure_openai_endpoint, azure_openai_embedding_deployment, azure_openai_key):\n",
    "    \"\"\"Defines the embedding skill for generating embeddings via Azure OpenAI.\"\"\"\n",
    "    return AzureOpenAIEmbeddingSkill(\n",
    "        description=\"Skill to generate embeddings via Azure OpenAI\",\n",
    "        context=\"/document\",\n",
    "        resource_uri=azure_openai_endpoint,\n",
    "        deployment_id=azure_openai_embedding_deployment,\n",
    "        model_name=AzureOpenAIModelName.TEXT_EMBEDDING3_LARGE,\n",
    "        api_key=azure_openai_key,\n",
    "        inputs=[\n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/description\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            OutputFieldMappingEntry(name=\"embedding\")\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def create_skillset(client, skillset_name, embedding_skill):\n",
    "    \"\"\"Creates or updates the skillset with an embedding skill.\"\"\"\n",
    "    skillset = SearchIndexerSkillset(\n",
    "        name=skillset_name,\n",
    "        description=\"Skillset for generating embeddings\",\n",
    "        skills=[embedding_skill],\n",
    "    )\n",
    "    try:\n",
    "        client.create_or_update_skillset(skillset)\n",
    "        print(f\"{skillset.name} created\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create or update skillset {skillset_name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "skillset_name = f\"{INDEX_NAME}-skillset\"\n",
    "client = SearchIndexerClient(endpoint=SEARCH_SERVICE_ENDPOINT, credential=azure_search_credential)\n",
    "\n",
    "embedding_skill = create_embedding_skill(AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME, AZURE_OPENAI_API_KEY)\n",
    "\n",
    "create_skillset(client, skillset_name, embedding_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv-sample-indexer created or updated.\n",
      "csv-sample-indexer is running. If queries return no results, please wait a bit and try again.\n"
     ]
    }
   ],
   "source": [
    "def create_and_run_indexer(\n",
    "    indexer_client, indexer_name, skillset_name, index_name, data_source_name\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an indexer, applies it to a given index, and runs the indexing process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        indexer = SearchIndexer(\n",
    "            name=indexer_name,\n",
    "            description=\"Indexer to index documents and generate embeddings\",\n",
    "            skillset_name=skillset_name,\n",
    "            target_index_name=index_name,\n",
    "            data_source_name=data_source_name,\n",
    "            # Indexing parameters to correctly parse CSV files\n",
    "            parameters=IndexingParameters(\n",
    "                batch_size=100,  # Adjust based on your content size and requirements\n",
    "                configuration=IndexingParametersConfiguration(\n",
    "                    parsing_mode=BlobIndexerParsingMode.DELIMITED_TEXT,\n",
    "                    first_line_contains_headers=True,\n",
    "                    query_timeout=None,\n",
    "                ),\n",
    "            ),\n",
    "            output_field_mappings=[FieldMapping(source_field_name=\"/document/embedding\", target_field_name=\"vector\")]\n",
    "        )\n",
    "\n",
    "        # Create or update the indexer\n",
    "        indexer_client.create_or_update_indexer(indexer)\n",
    "        print(f\"{indexer_name} created or updated.\")\n",
    "\n",
    "        # Run the indexer\n",
    "        indexer_client.run_indexer(indexer_name)\n",
    "        print(\n",
    "            f\"{indexer_name} is running. If queries return no results, please wait a bit and try again.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create or run indexer {indexer_name}: {e}\")\n",
    "\n",
    "# Main workflow\n",
    "data_source_name = f\"{INDEX_NAME}-blob\"\n",
    "indexer_name = f\"{INDEX_NAME}-indexer\"\n",
    "indexer_client = SearchIndexerClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT, credential=azure_search_credential\n",
    ")\n",
    "\n",
    "create_and_run_indexer(\n",
    "    indexer_client, indexer_name, skillset_name, INDEX_NAME, data_source_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poll for indexer completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer 'csv-sample-indexer' finished with status 'success'.\n"
     ]
    }
   ],
   "source": [
    "indexer_last_result = indexer_client.get_indexer_status(indexer_name).last_result\n",
    "indexer_status = IndexerExecutionStatus.IN_PROGRESS if indexer_last_result is None  else indexer_last_result.status\n",
    "\n",
    "while(indexer_status == IndexerExecutionStatus.IN_PROGRESS):\n",
    "    indexer_last_result = indexer_client.get_indexer_status(indexer_name).last_result\n",
    "    indexer_status = IndexerExecutionStatus.IN_PROGRESS if indexer_last_result is None  else indexer_last_result.status\n",
    "    print(f\"Indexer '{indexer_name}' is still running. Current status: '{indexer_status}'.\")\n",
    "\n",
    "print(f\"Indexer '{indexer_name}' finished with status '{indexer_status}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: World Briefings\n",
      "description: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\n",
      "label: World\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"What did Prime Minister Tony Blair say about climate change?\"  \n",
    "\n",
    "search_client = SearchClient(SEARCH_SERVICE_ENDPOINT, INDEX_NAME, credential=azure_search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = VectorizedQuery(vector=generate_embeddings(query), k_nearest_neighbors=3, fields=\"vector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    top=1\n",
    ")  \n",
    "\n",
    "for result in results:  \n",
    "    print(f\"title: {result['title']}\")  \n",
    "    print(f\"description: {result['description']}\")  \n",
    "    print(f\"label: {result['label']}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform RAG Using Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Assistant (GPT-4): Prime Minister Tony Blair urged the international community to consider global warming a dire threat\n",
      "and to agree on a plan of action to curb the \"alarming\" growth of greenhouse gases [doc1].\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        },\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": SEARCH_SERVICE_ENDPOINT,\n",
    "                    \"index_name\": INDEX_NAME,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": SEARCH_SERVICE_API_KEY,\n",
    "                    },\n",
    "                    \"query_type\": \"vector_semantic_hybrid\",\n",
    "                    \"embedding_dependency\": {\n",
    "                        \"type\": \"deployment_name\",\n",
    "                        \"deployment_name\": AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "                    },\n",
    "                    \"semantic_configuration\": \"mySemanticConfig\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "import textwrap\n",
    "if completion.choices:\n",
    "    message_content = completion.choices[0].message.content\n",
    "    wrapped_message_content = textwrap.fill(message_content, width=100)\n",
    "    print(f\"AI Assistant (GPT-4): {wrapped_message_content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
