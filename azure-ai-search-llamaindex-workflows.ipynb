{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows in LlamaIndex\n",
    "A Workflow in LlamaIndex is an event-driven framework that allows you to chain together different computational steps to achieve complex tasks, such as building a retrieval-augmented generation (RAG) system. The workflow is composed of steps, where each step handles specific types of events and can emit new events.\n",
    "\n",
    "Workflows are designed to be asynchronous and event-driven, meaning that each step only runs when the appropriate event is triggered. This allows for the creation of complex, multi-step processes that can be easily managed and monitored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-identity\n",
    "!pip install azure-search-documents==11.4.0\n",
    "!pip install -U llama-index\n",
    "!pip install llama-index-embeddings-azure-openai\n",
    "!pip install llama-index-llms-azure-openai\n",
    "!pip install llama-index-vector-stores-azureaisearch\n",
    "!pip install nest-asyncio\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Components of a Workflow\n",
    "## Events\n",
    "Events are the fundamental objects that are passed between the steps of a workflow. Events can carry data and signal to other steps that certain actions need to be taken. There are special events like `StartEvent` and `StopEvent`, and you can also define custom events to carry specific types of data.\n",
    "\n",
    "In the example provided, two custom events are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement\n",
    "from llama_index.core.callbacks import LlamaDebugHandler\n",
    "from llama_index.vector_stores.azureaisearch import (\n",
    "    IndexManagement,\n",
    "    MetadataIndexFieldType,\n",
    "    \n",
    ")\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.workflow.context import Context\n",
    "from llama_index.core.workflow.decorators import step\n",
    "from llama_index.core.workflow.drawing import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "from llama_index.core.workflow.errors import (\n",
    "    WorkflowRuntimeError,\n",
    "    WorkflowTimeoutError,\n",
    "    WorkflowValidationError,\n",
    ")\n",
    "from llama_index.core.workflow.events import Event, StartEvent, StopEvent\n",
    "from llama_index.core.workflow.workflow import Workflow\n",
    "from llama_index.core.workflow.context import Context\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core import get_response_synthesizer\n",
    "import pprint\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Environment Variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\") # I'm using GPT-3.5-turbo\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\") # I'm using text-embedding-ada-002\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "INDEX_NAME = \"llamindex-workflow-demo\"\n",
    "\n",
    "# Initialize Azure OpenAI and embedding models\n",
    "llm = AzureOpenAI(\n",
    "    model=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "# Initialize search clients\n",
    "credential = AzureKeyCredential(SEARCH_SERVICE_API_KEY)\n",
    "index_client = SearchIndexClient(endpoint=SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client = SearchClient(endpoint=SEARCH_SERVICE_ENDPOINT, index_name=INDEX_NAME, credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = AzureAISearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    # filterable_metadata_field_keys=metadata_fields,\n",
    "    index_name=INDEX_NAME,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS, # use VALIDATE to validate the index schema if using one that already exists\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"chunk\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    embedding_dimensionality=1536,  # Ensure this matches your embedding model\n",
    "    metadata_string_field_key=\"metadata\",\n",
    "    doc_id_field_key=\"doc_id\",\n",
    "    language_analyzer=\"en.lucene\",\n",
    "    vector_algorithm_type=\"exhaustiveKnn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Existing Index\n",
    "If you have an existing index, use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    [],\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Components of a Workflow\n",
    "## Events\n",
    "Events are the fundamental objects that are passed between the steps of a workflow. Events can carry data and signal to other steps that certain actions need to be taken. There are special events like `StartEvent` and `StopEvent`, and you can also define custom events to carry specific types of data.\n",
    "\n",
    "In the example provided, two custom events are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    \"\"\"Result of running reranking on retrieved nodes\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RetrieverEvent: This event carries the nodes retrieved from the vector store.\n",
    "- RerankEvent: This event carries the nodes after they have been reranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "Steps are individual units of work within a workflow. Each step is defined as an asynchronous function and decorated with @step(). The decorator automatically handles the input and output types for validation and ensures that each step runs only when the appropriate event is ready.\n",
    "\n",
    "In the provided workflow, steps are defined to ingest documents, retrieve relevant information, rerank the retrieved nodes, and synthesize a final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    @step(pass_context=True)\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest a document, triggered by a StartEvent with `dirname`.\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        ctx.data[\"index\"] = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embed_model,\n",
    "            storage_context=storage_context,\n",
    "        )\n",
    "        return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ingest Step** The ingest step handles the StartEvent to load and index documents from a specified directory. It uses the SimpleDirectoryReader to load the documents and then creates a VectorStoreIndex using these documents and the embedding model. This index is stored in the workflow's context for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowValidationError",
     "evalue": "To decorate retrieve please pass a workflow instance to the @step() decorator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWorkflowValidationError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129;43m@step\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpass_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;43;01masync\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mretrieve\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mContext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mev\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mStartEvent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRetrieverEvent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEntry point for RAG, triggered by a StartEvent with `query`.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dev\\azure-ai-search-python-playground\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\decorators.py:36\u001b[0m, in \u001b[0;36mstep.<locals>.decorator\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m workflow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo decorate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m please pass a workflow instance to the @step() decorator.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowValidationError(msg)\n\u001b[0;32m     37\u001b[0m     workflow\u001b[38;5;241m.\u001b[39madd_step(func)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# This will raise providing a message with the specific validation failure\u001b[39;00m\n",
      "\u001b[1;31mWorkflowValidationError\u001b[0m: To decorate retrieve please pass a workflow instance to the @step() decorator."
     ]
    }
   ],
   "source": [
    "@step(pass_context=True)\n",
    "async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent | None:\n",
    "    \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n",
    "    query = ev.get(\"query\")\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    print(f\"Query the database with: {query}\")\n",
    "\n",
    "    # store the query in the global context\n",
    "    ctx.data[\"query\"] = query\n",
    "\n",
    "    # get the index from the global context\n",
    "    index = ctx.data.get(\"index\")\n",
    "    if index is None:\n",
    "        print(\"Index is empty, load some documents before querying!\")\n",
    "        return None\n",
    "\n",
    "    retriever = index.as_retriever(similarity_top_k=2)\n",
    "    nodes = retriever.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "    return RetrieverEvent(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrieve Step**: The retrieve step is triggered by another StartEvent that contains a query. It retrieves relevant nodes from the indexed documents using a retriever configured with a similarity search (similarity_top_k=2). The retrieved nodes are then passed along in a RetrieverEvent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(pass_context=True)\n",
    "async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n",
    "    # Rerank the nodes\n",
    "    ranker = LLMRerank(choice_batch_size=5, top_n=3, llm=llm)\n",
    "    print(ctx.data.get(\"query\"), flush=True)\n",
    "    new_nodes = ranker.postprocess_nodes(ev.nodes, query_str=ctx.data.get(\"query\"))\n",
    "    print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "    return RerankEvent(nodes=new_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerank Step: The rerank step takes the nodes from the RetrieverEvent and reranks them using a language model (LLM). The reranked nodes are then passed along in a RerankEvent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(pass_context=True)\n",
    "async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n",
    "    \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "    summarizer = CompactAndRefine(llm=llm, streaming=True, verbose=True)\n",
    "    query = ctx.data.get(\"query\")\n",
    "\n",
    "    response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "    return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesize Step: The synthesize step generates a final response by synthesizing the reranked nodes into a coherent answer using another LLM. The result is then returned in a StopEvent, which stops the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "The **Context** object allows steps in the workflow to share data. In the example, the context is used to store the index created in the ingest step and the query provided in the retrieve step. This context ensures that the necessary data is available throughout the workflow's execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Execution\n",
    "Once the workflow steps are defined, the workflow can be executed by creating an instance of the workflow and calling its run method. The run method is asynchronous and must be awaited. Each step in the workflow is executed in sequence, with data being passed between them using events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Workflow\n",
    "w = RAGWorkflow()\n",
    "\n",
    "# Ingest the documents (example with a directory 'data')\n",
    "await w.run(dirname=\"data/txt\")\n",
    "\n",
    "# Run a query\n",
    "result = await w.run(query=\"How was Llama2 trained?\")\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case:\n",
    "- **Ingestion:** The workflow is first run with a directory name to ingest documents into the vector index.\n",
    "- **Query Execution**: The workflow is then run again with a query to retrieve, rerank, and synthesize a response from the ingested documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This workflow is a powerful abstraction that allows you to chain together complex tasks, such as retrieval-augmented generation, in a structured and manageable way. By leveraging custom events, context, and steps, you can build workflows that are both flexible and scalable, all while maintaining clear and concise code structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: does the president have a plan for covid-19?\n",
      "Retrieved 1 nodes.\n",
      "does the president have a plan for covid-19?\n",
      "Reranked nodes to 1\n",
      "=== GPT-4o-Generated Response ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Yes, the president has outlined a plan for COVID-19, which includes steps to stay protected with vaccines and treatments, prepare for new variants, end the shutdown of schools and businesses, and continue vaccinating the world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Details of Source Documents ===\n",
      "\n",
      "file_path: c:\\Dev\\azure-ai-search-python-playground\\data\\txt\\state_of_the_union.txt\n",
      "\n",
      "Because of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \n",
      "we are moving forward safely, back to more normal routines.  \n",
      "\n",
      "We’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \n",
      "\n",
      "Just a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines. \n",
      "\n",
      "Under these new guidelines, most Americans in most of the country can now be mask free.   \n",
      "\n",
      "And based on the projections, more of the country will reach that point across the next couple of weeks. \n",
      "\n",
      "Thanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \n",
      "\n",
      "I know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19. \n",
      "\n",
      "We will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \n",
      "\n",
      "Here are four common sense steps as we move forward safely.  \n",
      "\n",
      "First, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection. \n",
      "\n",
      "We will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \n",
      "\n",
      "The scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do. \n",
      "\n",
      "We’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \n",
      "\n",
      "We’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \n",
      "\n",
      "And we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.  \n",
      "\n",
      "If you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \n",
      "\n",
      "We’re leaving no one behind or ignoring anyone’s needs as we move forward. \n",
      "\n",
      "And on testing, we have made hundreds of millions of tests available for you to order for free.   \n",
      "\n",
      "Even if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \n",
      "\n",
      "Second – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants. \n",
      "\n",
      "If necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.  \n",
      "\n",
      "And, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed. \n",
      "\n",
      "I cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.  \n",
      "\n",
      "Third – we can end the shutdown of schools and businesses. We have the tools we need. \n",
      "\n",
      "It’s time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \n",
      "\n",
      "We’re doing that here in the federal government. The vast majority of federal workers will once again work in person. \n",
      "\n",
      "Our schools are open. Let’s keep it that way. Our kids need to be in school. \n",
      "\n",
      "And with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \n",
      "\n",
      "We achieved this because we provided free vaccines, treatments, tests, and masks. \n",
      "\n",
      "Of course, continuing this costs money. \n",
      "\n",
      "I will soon send Congress a request. \n",
      "\n",
      "The vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \n",
      "\n",
      "Fourth, we will continue vaccinating the world.     \n",
      "\n",
      "We’ve sent 475 Million vaccine doses to 112 countries, more than any other nation. \n",
      "\n",
      "And we won’t stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together.\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define RAG Workflow with Reranking\n",
    "class RAGWorkflow(Workflow):\n",
    "    @step(pass_context=True)\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest a document, triggered by a StartEvent with `dirname`.\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        ctx.data[\"index\"] = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embed_model,\n",
    "            storage_context=storage_context,\n",
    "        )\n",
    "        return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> RetrieverEvent | None:\n",
    "        \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n",
    "        query = ev.get(\"query\")\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        print(f\"Query the database with: {query}\")\n",
    "\n",
    "        # store the query in the global context\n",
    "        ctx.data[\"query\"] = query\n",
    "\n",
    "        # get the index from the global context\n",
    "        index = ctx.data.get(\"index\")\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=10)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n",
    "        # Rerank the nodes\n",
    "        ranker = LLMRerank(\n",
    "            choice_batch_size=5, top_n=3, llm=llm\n",
    "        )\n",
    "        print(ctx.data.get(\"query\"), flush=True)\n",
    "        new_nodes = ranker.postprocess_nodes(\n",
    "            ev.nodes, query_str=ctx.data.get(\"query\")\n",
    "        )\n",
    "        print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "        return RerankEvent(nodes=new_nodes)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(llm=llm, streaming=True, verbose=True)\n",
    "        query = ctx.data.get(\"query\")\n",
    "\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)\n",
    "\n",
    "# Initialize Workflow\n",
    "w = RAGWorkflow()\n",
    "\n",
    "# Ingest the document (example with a specific file 'data/txt/state_of_the_union.txt')\n",
    "await w.run(dirname=\"data/txt\")\n",
    "\n",
    "# Run a query\n",
    "result = await w.run(query=\"does the president have a plan for covid-19?\")\n",
    "\n",
    "# Function to display custom response\n",
    "def display_custom_response(response):\n",
    "    print(\"=== GPT-4o-Generated Response ===\")\n",
    "    display_response(response)\n",
    "    print(\"\\n=== Details of Source Documents ===\\n\")\n",
    "    for node in response.source_nodes:\n",
    "        print(node.get_content(metadata_mode=MetadataMode.LLM))\n",
    "        print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "# Await and collect the full response\n",
    "final_response = \"\"\n",
    "async for chunk in result.async_response_gen():\n",
    "    final_response += chunk\n",
    "\n",
    "# Create a mock response object for display\n",
    "class MockResponse:\n",
    "    def __init__(self, response, source_nodes):\n",
    "        self.response = response\n",
    "        self.source_nodes = source_nodes\n",
    "\n",
    "mock_response = MockResponse(final_response, result.source_nodes)\n",
    "\n",
    "# Display the response using the custom display function\n",
    "display_custom_response(mock_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
