{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Retrieval Augmented Generation (RAG) Pipeline with LlamaIndex, Azure AI Search, Azure OpenAI, Literal AI, and RAGAS\n",
    "In this notebook, we will build a Retrieval Augmented Generation (RAG) system from scratch using LlamaIndex, Azure AI Search as the vector store, Azure OpenAI as the Large Language Model (LLM), and integrate Literal AI for logging, evaluation, and visualization. We will use RAGAS for evaluating our RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "First, we'll install the necessary libraries and set up our environment.\n",
    "### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install llama-index\n",
    "!pip install ragas\n",
    "!pip install azure-search-documents==11.5.2\n",
    "!pip install llama-index-vector-stores-azureaisearch\n",
    "!pip install llama-index-llms-azure-openai\n",
    "!pip install llama-index-embeddings-azure-openai\n",
    "!pip install literalai\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.aio import SearchIndexClient as AsyncSearchIndexClient \n",
    "from azure.search.documents.aio import SearchClient as AsyncSearchClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.schema import TextNode, ImageNode, NodeWithScore, MetadataMode\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.query_engine import CustomQueryEngine, SimpleMultiModalQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement, MetadataIndexFieldType\n",
    "import os\n",
    "import asyncio\n",
    "from azure.storage.blob.aio import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List\n",
    "from llama_index.core.schema import TextNode, MetadataMode\n",
    "import nest_asyncio\n",
    "from llama_index.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n",
    "# Example usage of MultimodalQueryEngine\n",
    "from llama_index.core.response.notebook_utils import display_response, display_query_and_multimodal_response, display_source_node\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core import get_response_synthesizer\n",
    "import os\n",
    "import time\n",
    "from literalai import LiteralClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables and Set Up Credentials\n",
    "We will load our credentials from a .env file or environment variables. Make sure you have your Azure OpenAI, Azure AI Search, and Literal AI credentials ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Environment Variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\")\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "INDEX_NAME = \"llamaindex-azure-aisearch-rag\"\n",
    "\n",
    "# Initialize LLM and Embedding models\n",
    "llm = AzureOpenAI(\n",
    "    model=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=\"2024-10-01-preview\"\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=\"2024-10-01-preview\"\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Initialize search clients\n",
    "credential = AzureKeyCredential(SEARCH_SERVICE_API_KEY)\n",
    "index_client = SearchIndexClient(endpoint=SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client = SearchClient(endpoint=SEARCH_SERVICE_ENDPOINT, index_name=INDEX_NAME, credential=credential)\n",
    "\n",
    "async_index_client = AsyncSearchIndexClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT, \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "async_search_client = AsyncSearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "LITERAL_API_KEY = os.getenv('LITERAL_API_KEY')\n",
    "\n",
    "# Initialize Literal AI client\n",
    "literalai_client = LiteralClient(api_key=LITERAL_API_KEY)\n",
    "literalai_client.instrument_llamaindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "We'll load the documents we want to index and prepare them for the RAG system.\n",
    "\n",
    "### Load Documents\n",
    "Assuming you have a directory data/pdf containing your PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents \n",
    "documents = SimpleDirectoryReader('data/pdf').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG Pipeline\n",
    "We'll create an index using LlamaIndex with Azure AI Search as the vector store, and configure Azure OpenAI as the LLM.\n",
    "### Set Up the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = AzureAISearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    index_name=INDEX_NAME,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"chunk\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    embedding_dimensionality=1536, \n",
    "    metadata_string_field_key=\"metadata\",\n",
    "    doc_id_field_key=\"doc_id\",\n",
    "    language_analyzer=\"en.lucene\",\n",
    "    vector_algorithm_type=\"exhaustiveKnn\",\n",
    "    compression_type=\"scalar\" # Option to use \"scalar\" or \"binary\". NOTE: compression is only supported for HNSW\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create index with text splitter\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, your benefits cover scuba diving lessons as part of the PerksPlus program."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Does my benefits cover scuba diving?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_vector_store = AzureAISearchVectorStore(\n",
    "    search_or_index_client=async_search_client,\n",
    "    index_management=IndexManagement.VALIDATE_INDEX,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"chunk\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    embedding_dimensionality=1536, \n",
    "    metadata_string_field_key=\"metadata\",\n",
    "    doc_id_field_key=\"doc_id\",\n",
    "    language_analyzer=\"en.lucene\",\n",
    "    vector_algorithm_type=\"exhaustiveKnn\",\n",
    "    compression_type=\"scalar\" # Option to use \"scalar\" or \"binary\". NOTE: compression is only supported for HNSW\n",
    ")\n",
    "\n",
    "async_storage_context = StorageContext.from_defaults(vector_store=async_vector_store)\n",
    "\n",
    "# Create index with text splitter\n",
    "async_index = VectorStoreIndex.from_documents(\n",
    "    [],\n",
    "    storage_context=async_storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_query_engine = async_index.as_query_engine(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async query response: The main idea of the essay revolves around understanding the primary and secondary payer rules of the Northwind Health Plus plan, including the exceptions that apply when individuals have other health coverage. It emphasizes the importance of knowing one's coverage type, keeping track of medical expenses, understanding the rules of each plan, being aware of claim filing deadlines, and seeking assistance when needed to ensure proper coverage and timely processing of claims.\n"
     ]
    }
   ],
   "source": [
    "# Use the async query engine\n",
    "import asyncio\n",
    "nest_asyncio.apply()\n",
    "response = asyncio.run(async_query_engine.aquery(\"What is the main idea of the essay?\"))\n",
    "print(\"Async query response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Evaluation Dataset\n",
    "We'll create an evaluation dataset consisting of questions and their corresponding ground truth answers. This dataset will be used to evaluate our RAG system's performance.\n",
    "### Create Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "evaluation_data = [\n",
    "    {\n",
    "        \"question\": \"What are out-of-network providers and what are the implications of receiving care from them?\",\n",
    "        \"ground_truth\": \"Out-of-network providers are those who have not contracted with Northwind Health. As a result, they are not required to accept the amount of payment offered by Northwind Health, meaning that the patient may be responsible for a greater portion of the cost. Additionally, out-of-network providers may not offer additional services or discounts that are available to in-network providers.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why is it important to ensure that a healthcare provider is in-network?\",\n",
    "        \"ground_truth\": \"It is important to ensure that a healthcare provider is in-network because while it is possible to receive care from out-of-network providers, it is important to understand that the patient will be responsible for a greater portion of the costs. Choosing an in-network provider whenever possible is recommended to ensure the best value for health care expenses.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Under what circumstances can one receive care from out-of-network providers?\",\n",
    "        \"ground_truth\": \"If one is unable to find an in-network provider in their area or if they require a specific type of care that is not available from an in-network provider, they may receive care from an out-of-network provider. However, in these cases, the cost of care may be more expensive and the patient may be responsible for a greater portion of the costs.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does the deductible roll over into the next year?\",\n",
    "        \"ground_truth\": \"No, the deductible does not roll over into the next year. This means that the deductible amount must be met in the current year before the insurance begins to pay.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When does the deductible not apply to services?\",\n",
    "        \"ground_truth\": \"The deductible may not apply to all services. For example, it may not apply when receiving in-network emergency services.\"\n",
    "    },\n",
    "    # Harder RAG Questions\n",
    "    {\n",
    "        \"question\": \"What is the coverage policy for mental health treatment under the Northwind Standard plan?\",\n",
    "        \"ground_truth\": \"Northwind Standard does not cover mental health services, including therapy and psychiatric care, unless specifically mentioned in the policy. Coverage for mental health treatment may require additional approval or separate insurance.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the potential reasons for Northwind Health to deny coverage for a non-emergency out-of-network service?\",\n",
    "        \"ground_truth\": \"Northwind Health may deny coverage for non-emergency out-of-network services if they are deemed medically unnecessary, if preauthorization was not obtained, or if the service is not covered under the plan’s benefits.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What steps should an individual take if they wish to appeal a denied claim for an out-of-network provider under the Northwind Standard plan?\",\n",
    "        \"ground_truth\": \"To appeal a denied claim for an out-of-network provider, an individual should gather any relevant medical records and documentation, contact Northwind Health’s claims department, and submit a formal appeal. The appeal will be reviewed by a medical review board to determine if the claim meets coverage criteria.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does Northwind Health determine medical necessity for out-of-network services?\",\n",
    "        \"ground_truth\": \"Northwind Health determines medical necessity for out-of-network services based on clinical guidelines, patient history, and the severity of the condition. If the service is deemed not medically necessary, it may not be covered under the plan.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the impact of receiving care from an out-of-network provider on the Northwind Standard plan’s coverage for prescription drugs?\",\n",
    "        \"ground_truth\": \"Receiving care from an out-of-network provider does not directly affect prescription drug coverage, but prescriptions filled outside the network may incur higher out-of-pocket costs. Additionally, certain medications may not be covered under out-of-network plans.\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Evaluation Data to Literal AI\n",
    "We'll create a dataset in Literal AI and upload our evaluation data. This will allow us to compare different experiments later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset in Literal AI\n",
    "DATASET_NAME = \"Contoso-HR Evaluation Dataset\"\n",
    "literal_dataset = literalai_client.api.create_dataset(name=DATASET_NAME)\n",
    "\n",
    "# Upload data to Literal AI dataset\n",
    "for item in evaluation_data:\n",
    "    literal_dataset.create_item(\n",
    "        input={\"content\": item[\"question\"]},\n",
    "        expected_output={\"content\": item[\"ground_truth\"]}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for RAGAS Evaluation\n",
    "We'll transform the evaluation data into a format that RAGAS understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataset items from Literal AI\n",
    "experiment_items = literal_dataset.items\n",
    "\n",
    "# Extract questions and ground truths\n",
    "questions = []\n",
    "ground_truths = []\n",
    "\n",
    "for item in experiment_items:\n",
    "    question = item.input[\"content\"]\n",
    "    ground_truth = item.expected_output[\"content\"]\n",
    "    questions.append(question)\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'ground_truth': ground_truths\n",
    "}\n",
    "# Convert dict to dataset\n",
    "# To escape the issue with evaluate function later. Dataset should be of this dataset type, not just dict.\n",
    "data_samples_set = Dataset.from_dict(data_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the RAG System with RAGAS\n",
    "We'll use RAGAS to evaluate our RAG system using the prepared dataset and selected metrics.\n",
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation for Different Top K Retrieval Values\n",
    "We will run evaluations for different values of top_k: 1, 3, 5, 10, 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of top_k values to test\n",
    "top_k_values = [1, 3, 5, 10, 50]\n",
    "\n",
    "# Initialize a list to store overall experiment results\n",
    "overall_results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluations and Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for top_k = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597ad0a278b340b99a05adc5dbd72ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Query Engine:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e9b32f2db648ffb190eb038f71e6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for top_k = 1\n",
      "\n",
      "Starting evaluation for top_k = 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a37d8a7cba947e682d5d910ba3f50c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Query Engine:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b760fe3fb304fc2847a47e7b8b97658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for top_k = 3\n",
      "\n",
      "Starting evaluation for top_k = 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7c765df73f4157b199fe64a119e5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Query Engine:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4e057e1c3d4a66b4fa23d334ff58a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for top_k = 5\n",
      "\n",
      "Starting evaluation for top_k = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22476f58ede3427cb0674192505167b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Query Engine:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5125ad9c09406ab1560ca5a30458ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "C:\\Users\\fsunavala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3002: RuntimeWarning: coroutine 'Dispatcher.span.<locals>.async_wrapper' was never awaited\n",
      "  params = OrderedDict((param.name, param) for param in parameters)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for top_k = 10\n",
      "\n",
      "Starting evaluation for top_k = 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9e343a664747f4857142a80a9468be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Query Engine:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ea3c3a0d0542b3994581394b885b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n",
      "n values greater than 1 not support for LlamaIndex LLMs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for top_k = 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define top_k values to test\n",
    "top_k_values = [1, 3, 5, 10, 50]\n",
    "overall_results = []\n",
    "\n",
    "from ragas.integrations.llama_index import evaluate as ragas_evaluate\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    print(f\"Starting evaluation for top_k = {top_k}\")\n",
    "    \n",
    "    # Configure async query engine with current top_k\n",
    "    async_query_engine = async_index.as_query_engine(similarity_top_k=top_k)\n",
    "\n",
    "    # Run RAGAS evaluation\n",
    "    evaluation_results = ragas_evaluate(\n",
    "        query_engine=async_query_engine,\n",
    "        dataset=data_samples_set,\n",
    "        metrics=metrics,\n",
    "        llm=llm,\n",
    "        embeddings=embed_model\n",
    "    )\n",
    "    \n",
    "    results_df = evaluation_results.to_pandas()\n",
    "    \n",
    "    # Log to Literal AI\n",
    "    experiment = literal_dataset.create_experiment(\n",
    "        name=f\"Experiment - Top {top_k} retrieval\",\n",
    "        params=[{\"top_k\": top_k}]\n",
    "    )\n",
    "    \n",
    "    # Log individual results\n",
    "    for i, row in results_df.iterrows():\n",
    "        scores = [{\n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } for metric in metrics]\n",
    "        \n",
    "        experiment.log({\n",
    "            \"datasetItemId\": experiment_items[i].id,\n",
    "            \"scores\": scores,\n",
    "            \"input\": {\"question\": row[\"question\"]},\n",
    "            \"output\": {\"content\": row[\"answer\"]}\n",
    "        })\n",
    "\n",
    "    print(f\"Completed evaluation for top_k = {top_k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Evaluation Results to Literal AI\n",
    "We'll log the evaluation results to Literal AI for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Evaluation Results\n",
    "We can now analyze the evaluation results by printing the metrics and reviewing them in the Literal AI dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are out-of-network providers and what are...</td>\n",
       "      <td>0.762540</td>\n",
       "      <td>0.972039</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is it important to ensure that a healthcar...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Under what circumstances can one receive care ...</td>\n",
       "      <td>0.958039</td>\n",
       "      <td>0.991719</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the deductible roll over into the next year?</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852857</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When does the deductible not apply to services?</td>\n",
       "      <td>0.625751</td>\n",
       "      <td>0.670904</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the coverage policy for mental health ...</td>\n",
       "      <td>0.711286</td>\n",
       "      <td>0.496846</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the potential reasons for Northwind H...</td>\n",
       "      <td>0.973605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What steps should an individual take if they w...</td>\n",
       "      <td>0.992534</td>\n",
       "      <td>0.838581</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does Northwind Health determine medical ne...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.717583</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the impact of receiving care from an o...</td>\n",
       "      <td>0.786281</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  answer_relevancy  \\\n",
       "0  What are out-of-network providers and what are...          0.762540   \n",
       "1  Why is it important to ensure that a healthcar...          1.000000   \n",
       "2  Under what circumstances can one receive care ...          0.958039   \n",
       "3  Does the deductible roll over into the next year?          1.000000   \n",
       "4    When does the deductible not apply to services?          0.625751   \n",
       "5  What is the coverage policy for mental health ...          0.711286   \n",
       "6  What are the potential reasons for Northwind H...          0.973605   \n",
       "7  What steps should an individual take if they w...          0.992534   \n",
       "8  How does Northwind Health determine medical ne...          1.000000   \n",
       "9  What is the impact of receiving care from an o...          0.786281   \n",
       "\n",
       "   context_precision  context_recall  faithfulness  \n",
       "0           0.972039            1.00      1.000000  \n",
       "1           1.000000            1.00      1.000000  \n",
       "2           0.991719            1.00      0.916667  \n",
       "3           0.852857            1.00      1.000000  \n",
       "4           0.670904            1.00      1.000000  \n",
       "5           0.496846            0.50      0.500000  \n",
       "6           1.000000            1.00      1.000000  \n",
       "7           0.838581            0.75      1.000000  \n",
       "8           0.717583            1.00      1.000000  \n",
       "9           0.829412            1.00      1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display evaluation metrics\n",
    "results_df[['question', 'answer_relevancy', 'context_precision', 'context_recall', 'faithfulness']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
