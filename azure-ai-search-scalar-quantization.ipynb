{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search Scalar Quantization \n",
    "\n",
    "In this notebook, I'll show you how to use Azure OpenAI Service to generate embeddings using the latest and highest performing model from OpenAI, `text-embedding-3-large` and how to store these in Azure AI Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents cohere python-dotenv azure-identity --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    ")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    SearchField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.indexes._generated.models import (\n",
    "    SearchField,\n",
    "    ScalarQuantizationCompressionConfiguration,\n",
    "    ScalarQuantizationParameters,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Cohere and Azure Credentials\n",
    "Before generating embeddings or interacting with Azure AI Search, we need to set up our credentials for both Cohere and Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mload_dotenv\u001b[49m()\n\u001b[0;32m      2\u001b[0m cohere_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOHERE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m co \u001b[38;5;241m=\u001b[39m cohere\u001b[38;5;241m.\u001b[39mClient(cohere_api_key)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "search_service_api_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "index_name = \"wikipedia-2023-11-embed-multilingual-v3-index\"\n",
    "credential = AzureKeyCredential(search_service_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred loading the dataset: ParquetConfig.__init__() got an unexpected keyword argument 'language'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['_id', 'url', 'title', 'text', 'emb']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred loading the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage (assuming successful loading):\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Access the first document in the training split\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dev\\azure-ai-search-python-playground\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2810\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dev\\azure-ai-search-python-playground\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2794\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2796\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2797\u001b[0m )\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Dev\\azure-ai-search-python-playground\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 580\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[1;32mc:\\Dev\\azure-ai-search-python-playground\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[1;34m(key, columns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['_id', 'url', 'title', 'text', 'emb']\""
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "# Specify dataset and language parameters\n",
    "dataset_name = \"Cohere/wikipedia-2023-11-embed-multilingual-v3\"\n",
    "language = \"simple\"  # Use Simple English Wikipedia subset\n",
    "split = \"train\"      # Load the training split\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    docs = datasets.load_dataset(dataset_name, language=language, split=split)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the dataset: {e}\")\n",
    "\n",
    "# Example usage (assuming successful loading):\n",
    "print(docs[\"train\"][0])  # Access the first document in the training split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_preprocess_dataset(dataset_url):\n",
    "    response = requests.get(dataset_url)\n",
    "    dataset = json.loads(response.content)\n",
    "\n",
    "    # Customize data transformation based on your dataset and Azure AI Search requirements\n",
    "    documents = []\n",
    "    for data in dataset:\n",
    "        # Example transformation (modify as needed)\n",
    "        document = {\n",
    "            \"id\": data[\"id\"],  # Assuming a unique identifier field exists\n",
    "            \"text\": data[\"text\"],  # Assuming a text field exists for searching\n",
    "            # Add other relevant fields for indexing\n",
    "        }\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Download and pre-process the dataset\n",
    "documents = download_and_preprocess_dataset(dataset_url)\n",
    "\n",
    "print(\"Downloaded and pre-processed\", len(documents), \"documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate what scale we should configure an Azure AI Search service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb'],\n",
      "    num_rows: 100000\n",
      "})\n",
      "Dataset Schema:\n",
      "{'_id': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'emb': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Print the Dataset Object Directly\n",
    "print(docs)\n",
    "\n",
    "# Dataset Schema - data types and properties of columns\n",
    "print(\"Dataset Schema:\")\n",
    "print(docs.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we have 646,424 documents and each document has 1 vector field at 1024 dimensions so we can now estimate the raw size of vectors that will assist us with selecting a SKU and Partition count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vector index: 458069333.3333333 bytes\n"
     ]
    }
   ],
   "source": [
    "def interpolate_overhead(dimensions):\n",
    "    \"\"\"\n",
    "    Interpolate the HNSW algorithm overhead percentage based on the provided dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    - dimensions: The dimensions of the vector field.\n",
    "    \n",
    "    Returns:\n",
    "    - The interpolated or exact algorithm overhead percentage.\n",
    "    \"\"\"\n",
    "    # Known data points for overhead percentages\n",
    "    overhead_data = [(96, 20), (200, 8), (768, 2), (1536, 1)]\n",
    "    \n",
    "    # Sort the list by dimensions to ensure it is in ascending order\n",
    "    overhead_data.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Check if the provided dimension is below or above known data points\n",
    "    if dimensions <= overhead_data[0][0]:\n",
    "        return overhead_data[0][1]\n",
    "    elif dimensions >= overhead_data[-1][0]:\n",
    "        return overhead_data[-1][1]\n",
    "    \n",
    "    # Linear interpolation for dimensions within the known data points\n",
    "    for i in range(1, len(overhead_data)):\n",
    "        if dimensions <= overhead_data[i][0]:\n",
    "            x0, y0 = overhead_data[i-1]\n",
    "            x1, y1 = overhead_data[i]\n",
    "            # Linear interpolation formula\n",
    "            return y0 + (y1 - y0) * (dimensions - x0) / (x1 - x0)\n",
    "\n",
    "def calculate_vector_index_size(num_documents, dimensions, algorithm, m=4, deleted_docs_ratio_percent=10):\n",
    "    \"\"\"\n",
    "    Calculate the size of a vector index on Azure Cognitive Search, considering the chosen algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_documents: The total number of documents.\n",
    "    - dimensions: The dimensions of the vector field.\n",
    "    - algorithm: The algorithm used ('HNSW' or 'ExhaustiveKnn').\n",
    "    - m: The HNSW parameter determining the number of bi-directional links per vector. Only used for HNSW.\n",
    "    - deleted_docs_ratio_percent: The estimated percentage of deleted or updated documents. Only used for HNSW.\n",
    "    \n",
    "    Returns:\n",
    "    - The size of the vector index in bytes.\n",
    "    \"\"\"\n",
    "    # Size of data type in bytes (Edm.Single)\n",
    "    size_of_data_type = 4\n",
    "    \n",
    "    # Raw size calculation\n",
    "    raw_size_bytes = num_documents * dimensions * size_of_data_type\n",
    "    \n",
    "    if algorithm == \"ExhaustiveKnn\":\n",
    "        # For ExhaustiveKnn, the raw size is the estimate\n",
    "        return raw_size_bytes\n",
    "    elif algorithm == \"HNSW\":\n",
    "        # Perform linear interpolation to estimate algorithm overhead percentage\n",
    "        algorithm_overhead_percent = interpolate_overhead(dimensions)\n",
    "        \n",
    "        # Calculate total size considering algorithm overhead and deleted documents ratio for HNSW\n",
    "        total_size_bytes = raw_size_bytes * (1 + algorithm_overhead_percent / 100) * (1 + deleted_docs_ratio_percent / 100)\n",
    "        \n",
    "        return total_size_bytes\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm selected. Choose 'HNSW' or 'ExhaustiveKnn'.\")\n",
    "\n",
    "# Example usage\n",
    "num_documents = 100000  # Number of documents\n",
    "dimensions = 1024  # An example dimension not directly listed in the overhead data\n",
    "algorithm = \"HNSW\"  # Algorithm: \"HNSW\" or \"ExhaustiveKnn\"\n",
    "\n",
    "size_bytes = calculate_vector_index_size(num_documents, dimensions, algorithm)\n",
    "print(f\"Size of the vector index: {size_bytes} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above estinated vector index size of 2.96 GB, it seems that we can safely index on a Basic service at 1 partition as the max storage for this is 15 GB on my newly created service on April 3, 2024. See https://review.learn.microsoft.com/en-us/azure/search/search-limits-quotas-capacity?branch=pr-en-us-269645#vector-limits-on-services-created-after-april-3-2024-in-supported-regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_index(client, index_name):\n",
    "\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchField(\n",
    "            name=\"url\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"title\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"text\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"embedding\",\n",
    "            type=\"Collection(Edm.Single)\",\n",
    "\n",
    "            vector_search_dimensions=1024,  # Adjust based on your actual vector dimensions\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "\n",
    "                name=\"my-vector-config\",\n",
    "                algorithm_configuration_name=\"my-hnsw\",\n",
    "                compression_configuration_name=\"my-scalar-quantization\",\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"my-hnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4, metric=\"cosine\", ef_construction=400, ef_search=500\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        compressions=[\n",
    "            ScalarQuantizationCompressionConfiguration(\n",
    "                name=\"my-scalar-quantization\",\n",
    "                rerank_with_original_vectors=True,\n",
    "                default_oversampling=10,\n",
    "                parameters=ScalarQuantizationParameters(\n",
    "                    quantized_data_type=\"int8\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "\n",
    "    client.create_or_update_index(index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Documents and Their Embeddings\n",
    "Finally, this function indexes the documents along with their int8 embeddings into Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [01:16<00:00, 1304.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the Hugging Face dataset to a list of dictionaries\n",
    "documents_to_index = docs.map(lambda example: {\n",
    "    \"id\": str(example[\"_id\"]),  # Ensures 'id' is a string\n",
    "    \"URL\": example[\"url\"],\n",
    "    \"title\": example[\"title\"],\n",
    "    \"text\": example[\"text\"],\n",
    "    \"embedding\": example[\"emb\"],  # Ensure this is in the format Azure expects\n",
    "}, remove_columns=docs.column_names)  # This removes original columns not needed for indexing\n",
    "\n",
    "# Convert to a list of dictionaries\n",
    "documents_to_index = documents_to_index.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished indexing documents.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def upload_embeddings_with_buffered_sender(endpoint, index_name, credential, documents):\n",
    "    # Initialize the SearchIndexingBufferedSender\n",
    "    batch_client = SearchIndexingBufferedSender(\n",
    "        endpoint=endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=credential\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Add upload actions for all documents in a single call\n",
    "        batch_client.upload_documents(documents=documents)\n",
    "\n",
    "        # Manually flush to send any remaining documents in the buffer\n",
    "        batch_client.flush()\n",
    "    except HttpResponseError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Clean up resources by closing the batch_client\n",
    "        batch_client.close()\n",
    "\n",
    "    print(\"Finished indexing documents.\")\n",
    "\n",
    "# Example usage, make sure endpoint, index_name, and credential are properly defined\n",
    "# Assuming credential is an instance of AzureKeyCredential\n",
    "# And documents_to_index is your list of documents to be indexed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField, SearchField, SearchFieldDataType, VectorSearch,\n",
    "    VectorSearchProfile, HnswAlgorithmConfiguration, VectorSearchAlgorithmKind,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "# Make sure to define or import your create_or_update_index function here\n",
    "\n",
    "# Initialize Azure Search Index Client for managing indexes\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=search_service_endpoint,\n",
    "    credential=credential,\n",
    "    index_name=index_name  # This parameter is not required here and should be omitted\n",
    ")\n",
    "\n",
    "\n",
    "# Create or update the search index to include the embedding field\n",
    "create_or_update_index(search_index_client, index_name)\n",
    "\n",
    "# Initialize the SearchClient for indexing documents\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_service_endpoint, \n",
    "    index_name=index_name, \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Assuming docs is already defined and loaded with your documents\n",
    "upload_embeddings_with_buffered_sender(search_service_endpoint, index_name, credential, documents_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, input_type=\"search_query\"):\n",
    "    model = \"embed-english-v3.0\"\n",
    "    # Ensure texts is a list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    response = co.embed(\n",
    "        texts=texts,\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        embedding_types=[\"int8\"],\n",
    "    )\n",
    "    return [embedding for embedding in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'EmbeddingsByType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfoundational figures in computer science\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate query embeddings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Use input_type=\"search_query\" for query embeddings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m search_client \u001b[38;5;241m=\u001b[39m SearchClient(search_service_endpoint, index_name, credential)\n\u001b[0;32m     12\u001b[0m vector_query \u001b[38;5;241m=\u001b[39m VectorizedQuery(\n\u001b[0;32m     13\u001b[0m     vector\u001b[38;5;241m=\u001b[39mquery_embeddings[\u001b[38;5;241m0\u001b[39m], k_nearest_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[1;34m(texts, input_type)\u001b[0m\n\u001b[0;32m      5\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [texts]\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39membed(\n\u001b[0;32m      8\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     10\u001b[0m     input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[0;32m     11\u001b[0m     embedding_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint8\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'EmbeddingsByType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Query for vector search\n",
    "query = \"foundational figures in computer science\"\n",
    "\n",
    "# Generate query embeddings\n",
    "# Use input_type=\"search_query\" for query embeddings\n",
    "query_embeddings = generate_embeddings(query, input_type=\"search_query\")\n",
    "\n",
    "search_client = SearchClient(search_service_endpoint, index_name, credential)\n",
    "\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings[0], k_nearest_neighbors=3, fields=\"embedding\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None,  # No search text for pure vector search\n",
    "    vector_queries=[vector_query],\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Title: {result['text']}\")\n",
    "    print(f\"Score: {result['@search.score']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
